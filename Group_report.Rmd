---
title: "ML01_Group15_Report"
author: "Zuxiang Li, Marcos F. Mourao, Agust√≠n Valencia"
date: "11/21/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(openxlsx)
library(kknn)
library(ggplot2)
library(MASS)
library(glmnet)
```

# Assignment 3. Feature selection by cross-validation in a linear model. 

### 1. Implement an R function that performs feature selection (best subset selection) in linear regression by using k-fold cross-validation without using any specialized function like lm() (**use only basic R functions**) your function should depend on: 
- $X$: Matrix containing X measurements.
- $Y$: Vector containing Y measurements.
- $Nfolds$: number of folds in the cross-validation.

You may assume in your code that matrix $X$ has 5 columns. The function should plot the CV scores computed for various feature subsets against the number of features, and it should  also return the optimal subset of features and the corresponding cross-validation (CV) score. Before splitting into folds, the data should be permuted and the seed 12345 should be used for that purpose. 




# Assignment 4. Linear regression and regularization

### 1. Import data and create a plot of Moisture versus Protein. Do you think these data are described well by a linear model? 

```{r, echo=FALSE, out.height='30%', fig.align='center'}
## Import data and plot Moisture vs Protein.
data <- read.xlsx("data/tecator.xlsx")
plot(data$Protein, data$Moisture)
```

By the plot, although there are some outliers, it seems that the data could be approximated by a linear model. 

### 2. Consider model $M_i$ in which Moisture is normally distributed and the expected Moisture is polynomial 

$$M_{i} = \sum_{j=0}^{i}{\beta_j x^j} + \varepsilon$$
$$i = {1,\cdots, 6} $$
$$\varepsilon \sim N(\mu,\sigma^2)$$

### 3. Divide the data (50/50) and fit models $M_i, i=1,\cdots, 6$. For each model, record the training and validation MSE and present a plot showing how training and validation MSE depend on $i$. Which model is best according to this plot? How do MSE values change and why? Interpret this picture in bias-variance tradeoff.

```{r, echo=FALSE, out.height='30%', fig.align='center'}
# Creating data sets
p <- data$Protein
y <- data$Moisture
P <- data.frame(
    Y = y,
    P1 = p, 
    P2 = p^2, 
    P3 = p^3, 
    P4 = p^4, 
    P5 = p^5, 
    P6 = p^6 
)

n = dim(P)[1]
set.seed(12345)
id = sample(1:n, floor(n*0.5))
train = P[id,]
test = P[-id,]

# Training models
M1 <- lm(Y ~ ., data = train[,1:2])
M2 <- lm(Y ~ ., data = train[,1:3])
M3 <- lm(Y ~ ., data = train[,1:4])
M4 <- lm(Y ~ ., data = train[,1:5])
M5 <- lm(Y ~ ., data = train[,1:6])
M6 <- lm(Y ~ ., data = train[,1:7])

## Train Scores
eval_model <- function(model, data) {
  pred <- predict(model, data)
  errors <- pred - data$Y
  MSE <- mean(errors^2)
  return(MSE)
}

MSE_train <- c()
MSE_train[1] <- eval_model(M1, train)
MSE_train[2] <- eval_model(M2, train)
MSE_train[3] <- eval_model(M3, train)
MSE_train[4] <- eval_model(M4, train)
MSE_train[5] <- eval_model(M5, train)
MSE_train[6] <- eval_model(M6, train)

MSE_test <- c()
MSE_test[1] <- eval_model(M1, test)
MSE_test[2] <- eval_model(M2, test)
MSE_test[3] <- eval_model(M3, test)
MSE_test[4] <- eval_model(M4, test)
MSE_test[5] <- eval_model(M5, test)
MSE_test[6] <- eval_model(M6, test)

df <- data.frame(
    degree <- c(1:6),
    MSE_train,
    MSE_test
)
p <- ggplot()
p <- p +geom_point(data = df, aes( x = degree, y = MSE_train, color="Train")) + 
    geom_line(data = df, aes( x = degree, y = MSE_train, color="Train"))
p <- p + geom_point(data = df, aes( x = degree, y = MSE_test, color="Test")) + 
    geom_line(data = df, aes( x = degree, y = MSE_test, color="Test"))
p <- p + labs(y="MSE", colour="Dataset", title = "Mean Square Errors") + geom_line() 
p

```

### 4. Perform variable selection of a linear model in which Fat is response and Channel1-Channel100 are predictors by using stepAIC. Comment on how many variables were selected.

```{r, echo=FALSE, results="hide"}
adhok_data <- data[,2:101]
Fat <- data$Fat
adhok_data <- cbind(adhok_data, Fat)
model <- lm(Fat ~ . , data=adhok_data) 
step <- stepAIC(model, direction ="both")
selected_vars <- step$coefficients
```

After running stepAIC we get that the amount of selected variables is :

```{r, echo=FALSE}
cat("There were selected", length(selected_vars), "variables\n")
```

Thus, taking into account that one of them is the intercept, we have 63 selected variables out of 100. 


### 5. Fit a Ridge regression model with the same predictor and response variables. Present a plot showing how model coefficients depend on the log of the penalty factor $\lambda$ and report how the coefficients change with $\lambda$

```{r, echo=FALSE, out.height='30%', fig.align='center'}
covariates <- scale(adhok_data[1:(length(adhok_data)-1)])
response <- scale(adhok_data$Fat)
model_ridge <- glmnet(covariates, response, alpha = 0, family = "gaussian")
plot(model_ridge, xvar="lambda", label=T)
```

It can be seen that when using Ridge regression among the $\lambda$ increasing, coefficients converge to zero, though the amount of parameters still 100 since Ridge do not drop them. 


### 6. Repeat step 5 but fit with LASSO instead of the Ridge regression and compare the plots from steps 5 and 6. Conclusions? 

```{r, echo=FALSE, out.height='30%', fig.align='center'}
model_lasso <- glmnet(covariates, response, alpha = 1, family = "gaussian")
plot(model_lasso, xvar="lambda", label=T)
```

LASSO converges to zero much faster than Ridge regression and it also drops variables. 


### 7. Use cross-validation to find optimal LASSO model (make sure that case $\lambda=0$ is also considered by the procedure), report the optimal $\lambda$ and how many variables were chosen by the model and make conclusions. Present also a plot showing the dependence of the CV score and comment how the CV score changes $\lambda$

```{r, echo=FALSE, out.height='30%', fig.align='center'}
lambdas <- append(model_lasso$lambda,0)
cv_model_lasso <- cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian", lambda=lambdas)
cat("The best performance was at lambda = ", cv_model_lasso$lambda.min, "\n")
cv_model_lasso
plot(cv_model_lasso)
```


From the cross-validated model it is seen that the  $\lambda$ value for which it is obtained the minimum MSE score is at $\lambda_{min} = 0$. The model has 100 non-zero parameters. Along $\log(\lambda)$ increases MSE also increases


### 8. Compare the results from steps 4 and 7.

For (4) after performing a stepAIC over the model 36 variables were dropped, getting a final model with only 63 variables (plus the intercept). Nonetheless, for (7) after cross-validating the LASSO model it has been found that the best performance regarding MSE scores is at $\lambda=0$ which implies no penalization, thus, no parameters will be dropped.

